# ------------------------------------------------------------------------------
#          ---        
#        / o o \    Snakemake workflow phylo-BEAST
#        V\ Y /V    Config file snakemake-phylo-BEAST
#    (\   / - \     
#     )) /    |     
#     ((/__) ||     Code by Ceci VA 
# ------------------------------------------------------------------------------

input: 
  metadata: "resources/data/metadata.tsv"
  sequences: "resources/data/sequences.fasta"
  
lapis:
  database: "open" # gisaid or open
  seq_id: "genbankAccession" #gisaidEpiIsl or genbankAccession
  access_key: "LAPIS_ACCESS_KEY" # only for gisaid endpoint

output:
  data_curation: false # False if you already have the alignment to run the analysis
  align: false
  lapis_import: true #TODO allow for mixing of lapis and user datasets
  mask: false
  phylogenetics: true
  phylodynamics: true


dataset:
  dataset1_lapis: 
    replicates: 3
    structure:
      deme1:
        select:
          nextcladeQcMissingDataScoreFrom: 0
          nextcladeQcMissingDataScoreTo: 300
          dateFrom: ""
          dateTo: ""
        subsample:
          n: -1
          method: "random" # none, random, weighted or uniform
          weights_file: null
          include: null
          exclude: null
      deme2:
        select:
          nextcladeQcMissingDataScoreFrom: 0
          nextcladeQcMissingDataScoreTo: 300
          dateFrom: ""
          dateTo: ""
    
    

  # dataset1: 
  #   select: "all"      
  #   filter:
  #     missing_proportion: 0.1
  #     min_length: 29000
  #   replicates: 3
  #   subsample:
  #     n: -1
  #     method: "random" # none, random, weighted or uniform
  #     weights_file: null
  #     include: null
  #     exclude: null

  # dataset1_structured:
  #   replicates: 3
  #   filter: true
  #   structure: 
  #     deme1: # all seqs in metadata from that type
  #       select: 
  #         type: "type1" # for = queries
  #       qc:
  #         missing_proportion: 0.1
  #         min_length: 29000
  #       subsample:
  #         n: -1
  #         method: "random" # none, random, weighted or uniform
  #         weights_file: null
  #         include: null
  #         exclude: null
  #     deme2: 
  #       select:  # from metadata file example 
  #         type: "type2" # for = queries
  #         # date: # for < or > queries
  #         #   min:
  #         #   max:
  #       qc:
  #         missing_proportion: 0.1
  #         min_length: 29000
  #       subsample:
  #         n: -1
  #         method: "random" # none, random, weighted or uniform
  #         weights_file: null
  #         include: null
  #         exclude: null


  # dataset1_structured:
  #   structure: 
  #     deme1: # all seqs in metadata from that type
  #       select:
  #         type: "type1" # for = queries
  #     deme2: 
  #       select: # from metadata file example 
  #         type: "type2" # for = queries
  #         # date: # for < or > queries
  #         #   min:
  #         #   max:
          
        
  # dataset2:
  #   replicates: 3
  #   select: # from metadata file example 
  #       date: # for < or > queries
  #         min:
  #         max:
  #       type: # for = queries
  #   subsample:
  #       n: -1
  #       method: "random" # none, random, weighted or uniform
  #       weights_file: null
  #       include: null
  #       exclude: null

  #   dataset2_structured: # with lapis import
  #   deme1:
  #     select: # lapis import example from id file
  #       ids_file: ""
  #       nextcladeQcMissingDataScoreFrom: 0
  #       nextcladeQcMissingDataScoreTo: 300
  #       dateFrom: ""
  #       dateTo: ""
  #     subsample:
  #       n: -1
  #       # method: "random" # none, random, weighted or uniform
  #       # weights_file: null
  #       include: null
  #       exclude: null
  #       # replicates: 1
        
  #   deme2:
  #     select: # lapis import example with subsample and replicates
  #       ids_file: ""
  #       nextcladeQcMissingDataScoreFrom: 0
  #       nextcladeQcMissingDataScoreTo: 300
  #       dateFrom: ""
  #       dateTo: ""
  #     subsample:
  #       n: -1
  #       method: "random" # none, random, weighted or uniform
  #       weights_file: null
  #       include: null
  #       exclude: null
  #       replicates: 1


align:
  parameters:

mask:
  mask_from_beginning: 100
  mask_from_end: 200
  mask_sites: [21846, 21987, 22992, 23012, 23063, 23604, 24410]

ml_tree:
  tree_args: " -m GTR -nstop 50 -ninit 50 -bb 1000 -redo" #--date TAXNAME -o 'EPI_ISL_406798|China|2019-12-26' for time tree
  threads: 5

beast:
  length: 10000000
  time: 7200
  mem_mb: 4096
  threads: 10
  burnin: 10
  chains: 2
  action: "overwrite"
  java: "/cluster/home/ceciliav/lib/beast/jre/bin/java"
  jar: "resources/BDMM-Prime-rho.jar" 

  run:
    analysis1:
      dataset: "dataset1"
      xml: "resources/beast_xml/example.xml"
      model_params:
        death_rate: 73.0

    analysis2:
      dataset: "dataset1"
      xml: "resources/beast_xml/example.xml"
      model_params:
        death_rate: 36.0

